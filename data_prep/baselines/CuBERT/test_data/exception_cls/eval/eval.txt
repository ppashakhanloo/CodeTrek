{"function": "@login_required\n@permission_required('workshops.add_person', raise_exception=True)\ndef person_bulk_add(request):\n    if request.method == 'POST':\n        form = PersonBulkAddForm(request.POST, request.FILES)\n        if form.is_valid():\n            charset = request.FILES['file'].charset or settings.DEFAULT_CHARSET\n            stream = io.TextIOWrapper(request.FILES['file'].file, charset)\n            try:\n                persons_tasks, empty_fields = upload_person_task_csv(stream)\n            except csv.Error as e:\n                messages.add_message(\n                    request, messages.ERROR,\n                    \"Error processing uploaded .CSV file: {}\".format(e))\n            except __HOLE__ as e:\n                messages.add_message(\n                    request, messages.ERROR,\n                    \"Please provide a file in {} encoding.\"\n                    .format(charset))\n            else:\n                if empty_fields:\n                    msg_template = (\"The following required fields were not\"\n                                    \" found in the uploaded file: {}\")\n                    msg = msg_template.format(', '.join(empty_fields))\n                    messages.add_message(request, messages.ERROR, msg)\n                else:\n                    # instead of insta-saving, put everything into session\n                    # then redirect to confirmation page which in turn saves\n                    # the data\n                    request.session['bulk-add-people'] = persons_tasks\n                    return redirect('person_bulk_add_confirmation')\n\n    else:\n        form = PersonBulkAddForm()\n\n    context = {\n        'title': 'Bulk Add People',\n        'form': form,\n        'charset': settings.DEFAULT_CHARSET,\n    }\n    return render(request, 'workshops/person_bulk_add_form.html', context)", "label": "UnicodeDecodeError", "info": "dataset/ETHPy150Open swcarpentry/amy/workshops/views.py/person_bulk_add"}
{"function": "@login_required\n@permission_required('workshops.add_person', raise_exception=True)\ndef person_bulk_add_confirmation(request):\n    \"\"\"\n    This view allows for manipulating and saving session-stored upload data.\n    \"\"\"\n    persons_tasks = request.session.get('bulk-add-people')\n\n    # if the session is empty, add message and redirect\n    if not persons_tasks:\n        messages.warning(request, \"Could not locate CSV data, please try the upload again.\")\n        return redirect('person_bulk_add')\n\n    if request.method == 'POST':\n        # update values if user wants to change them\n        personals = request.POST.getlist(\"personal\")\n        families = request.POST.getlist(\"family\")\n        usernames = request.POST.getlist(\"username\")\n        emails = request.POST.getlist(\"email\")\n        events = request.POST.getlist(\"event\")\n        roles = request.POST.getlist(\"role\")\n        data_update = zip(personals, families, usernames, emails, events,\n                          roles)\n        for k, record in enumerate(data_update):\n            personal, family, username, email, event, role = record\n            # \"field or None\" converts empty strings to None values\n            persons_tasks[k] = {\n                'personal': personal,\n                'family': family,\n                'username': username,\n                'email': email or None\n            }\n            # when user wants to drop related event they will send empty string\n            # so we should unconditionally accept new value for event even if\n            # it's an empty string\n            persons_tasks[k]['event'] = event\n            persons_tasks[k]['role'] = role\n            persons_tasks[k]['errors'] = None  # reset here\n\n        # save updated data to the session\n        request.session['bulk-add-people'] = persons_tasks\n\n        # check if user wants to verify or save, or cancel\n        if request.POST.get('verify', None):\n            # if there's \"verify\" in POST, then do only verification\n            any_errors = verify_upload_person_task(persons_tasks)\n            if any_errors:\n                messages.add_message(request, messages.ERROR,\n                                     \"Please make sure to fix all errors \"\n                                     \"listed below.\")\n\n            context = {'title': 'Confirm uploaded data',\n                       'persons_tasks': persons_tasks,\n                       'any_errors': any_errors}\n            return render(request, 'workshops/person_bulk_add_results.html',\n                          context)\n\n        # there must be \"confirm\" and no \"cancel\" in POST in order to save\n        elif (request.POST.get('confirm', None) and\n              not request.POST.get('cancel', None)):\n            try:\n                # verification now makes something more than database\n                # constraints so we should call it first\n                verify_upload_person_task(persons_tasks)\n                persons_created, tasks_created = \\\n                    create_uploaded_persons_tasks(persons_tasks)\n            except (IntegrityError, __HOLE__, InternalError) as e:\n                messages.add_message(request, messages.ERROR,\n                                     \"Error saving data to the database: {}. \"\n                                     \"Please make sure to fix all errors \"\n                                     \"listed below.\".format(e))\n                any_errors = verify_upload_person_task(persons_tasks)\n                context = {'title': 'Confirm uploaded data',\n                           'persons_tasks': persons_tasks,\n                           'any_errors': any_errors}\n                return render(request,\n                              'workshops/person_bulk_add_results.html',\n                              context, status=400)\n\n            else:\n                request.session['bulk-add-people'] = None\n                messages.add_message(\n                    request, messages.SUCCESS,\n                    'Successfully created {0} persons and {1} tasks.'\n                    .format(len(persons_created), len(tasks_created))\n                )\n                return redirect('person_bulk_add')\n\n        else:\n            # any \"cancel\" or no \"confirm\" in POST cancels the upload\n            request.session['bulk-add-people'] = None\n            return redirect('person_bulk_add')\n\n    else:\n        # alters persons_tasks via reference\n        any_errors = verify_upload_person_task(persons_tasks)\n\n        context = {'title': 'Confirm uploaded data',\n                   'persons_tasks': persons_tasks,\n                   'any_errors': any_errors}\n        return render(request, 'workshops/person_bulk_add_results.html',\n                      context)", "label": "ObjectDoesNotExist", "info": "dataset/ETHPy150Open swcarpentry/amy/workshops/views.py/person_bulk_add_confirmation"}
{"function": "@login_required\n@permission_required(['workshops.change_event', 'workshops.add_task'],\n                     raise_exception=True)\ndef event_edit(request, event_ident):\n    try:\n        event = Event.get_by_ident(event_ident)\n        tasks = event.task_set.order_by('role__name')\n    except __HOLE__:\n        raise Http404(\"No event found matching the query.\")\n\n    event_form = EventForm(prefix='event', instance=event)\n    task_form = TaskForm(prefix='task', initial={\n        'event': event,\n    })\n\n    if request.method == 'POST':\n        # check which form was submitted\n        if \"task-role\" in request.POST:\n            task_form = TaskForm(request.POST, prefix='task')\n\n            if task_form.is_valid():\n                task = task_form.save()\n\n                messages.success(\n                    request,\n                    '{event} was added a new task \"{task}\".'.format(\n                        event=str(event),\n                        task=str(task),\n                    ),\n                )\n\n                # if event.attendance is lower than number of learners, then\n                # update the attendance\n                update_event_attendance_from_tasks(event)\n\n                # to reset the form values\n                return redirect(request.path)\n\n            else:\n                messages.error(request, 'Fix errors below.')\n\n        else:\n            event_form = EventForm(request.POST, prefix='event',\n                                   instance=event)\n            if event_form.is_valid():\n                event = event_form.save()\n\n                messages.success(\n                    request,\n                    '{name} was updated successfully.'.format(\n                        name=str(event),\n                    ),\n                )\n\n                # if event.attendance is lower than number of learners, then\n                # update the attendance\n                update_event_attendance_from_tasks(event)\n\n                return redirect(event)\n\n            else:\n                messages.error(request, 'Fix errors below.')\n\n    context = {'title': 'Edit Event {0}'.format(event.get_ident()),\n               'event_form': event_form,\n               'object': event,\n               'model': Event,\n               'tasks': tasks,\n               'task_form': task_form,\n               'form_helper': bootstrap_helper,\n               'form_helper_with_add': bootstrap_helper_with_add,\n               }\n    return render(request, 'workshops/event_edit_form.html', context)", "label": "ObjectDoesNotExist", "info": "dataset/ETHPy150Open swcarpentry/amy/workshops/views.py/event_edit"}
{"function": "@login_required\n@permission_required('workshops.delete_event', raise_exception=True)\ndef event_delete(request, event_ident):\n    \"\"\"Delete event, its tasks and related awards.\"\"\"\n    try:\n        event = Event.get_by_ident(event_ident)\n        event.delete()\n\n        messages.success(request,\n                         'Event and its tasks were deleted successfully.')\n        return redirect(reverse('all_events'))\n    except __HOLE__:\n        raise Http404(\"No event found matching the query.\")\n    except ProtectedError as e:\n        return failed_to_delete(request, event, e.protected_objects)", "label": "ObjectDoesNotExist", "info": "dataset/ETHPy150Open swcarpentry/amy/workshops/views.py/event_delete"}
{"function": "@login_required\ndef event_import(request):\n    \"\"\"Read tags from remote URL and return them as JSON.\n\n    This is used to read tags from workshop website and then fill up fields\n    on event_create form.\"\"\"\n\n    # TODO: remove POST support completely\n    url = request.POST.get('url', '').strip()\n    if not url:\n        url = request.GET.get('url', '').strip()\n    try:\n        # fetch page\n        response = requests.get(url)\n        response.raise_for_status()  # assert it's 200 OK\n        content = response.text\n\n        # find tags\n        tags = find_tags_on_event_website(content)\n\n        if 'slug' not in tags:\n            # there are no HTML tags, so let's try the old method\n            index_url, repository = generate_url_to_event_index(url)\n\n            # fetch page\n            response = requests.get(index_url)\n\n            if response.status_code == 200:\n                # don't throw errors for pages we fall back to\n                content = response.text\n                tags = find_tags_on_event_index(content)\n\n                if 'slug' not in tags:\n                    tags['slug'] = repository\n\n        # normalize (parse) them\n        tags = parse_tags_from_event_website(tags)\n\n        return JsonResponse(tags)\n\n    except requests.exceptions.HTTPError as e:\n        return HttpResponseBadRequest(\n            'Request for \"{0}\" returned status code {1}.'\n            .format(url, e.response.status_code)\n        )\n\n    except requests.exceptions.RequestException:\n        return HttpResponseBadRequest('Network connection error.')\n\n    except WrongWorkshopURL as e:\n        return HttpResponseBadRequest(str(e))\n\n    except __HOLE__:\n        return HttpResponseBadRequest('Missing or wrong \"url\" parameter.')", "label": "KeyError", "info": "dataset/ETHPy150Open swcarpentry/amy/workshops/views.py/event_import"}
{"function": "@login_required\n@permission_required('workshops.add_invoicerequest', raise_exception=True)\ndef event_invoice(request, event_ident):\n    try:\n        event = Event.get_by_ident(event_ident)\n    except __HOLE__:\n        raise Http404(\"No event found matching the query.\")\n\n    form = InvoiceRequestForm(initial=dict(\n        organization=event.host, date=event.start, event=event,\n        event_location=event.venue, amount=event.admin_fee,\n    ))\n\n    if request.method == 'POST':\n        form = InvoiceRequestForm(request.POST)\n\n        if form.is_valid():\n            form.save()\n            messages.success(request,\n                             'Successfully added an invoice request for {}.'\n                             .format(event.get_ident()))\n            return redirect(reverse('event_details',\n                                    args=[event.get_ident()]))\n        else:\n            messages.error(request, 'Fix errors below.')\n\n    context = {\n        'title_left': 'Event {}'.format(event.get_ident()),\n        'title_right': 'New invoice request',\n        'event': event,\n        'form': form,\n        'form_helper': bootstrap_helper,\n    }\n    return render(request, 'workshops/event_invoice.html', context)", "label": "ObjectDoesNotExist", "info": "dataset/ETHPy150Open swcarpentry/amy/workshops/views.py/event_invoice"}
{"function": "@login_required\ndef object_changes(request, revision_id):\n    revision = get_object_or_404(Revision, pk=revision_id)\n\n    # we assume there's only one version per revision\n    current_version = revision.version_set.all()[0]\n    obj = current_version.object\n\n    try:\n        previous_version = get_for_object(obj) \\\n                                .filter(pk__lt=current_version.pk)[0]\n        obj_prev = previous_version.object\n    except __HOLE__:\n        # first revision for an object\n        previous_version = current_version\n        obj_prev = obj\n\n    context = {\n        'object_prev': obj_prev,\n        'object': obj,\n        'previous_version': previous_version,\n        'current_version': current_version,\n        'revision': revision,\n        'title': str(obj),\n    }\n    if obj.__class__ == Person:\n        return render(request, 'workshops/person_diff.html', context)\n    elif obj.__class__ == Event:\n        return render(request, 'workshops/event_diff.html', context)\n    else:\n        context['verbose_name'] = obj._meta.verbose_name\n        context['fields'] = [\n            f for f in obj._meta.get_fields()\n            if f.concrete and not f.is_relation\n        ]\n        return render(request, 'workshops/object_diff.html', context)\n\n# ------------------------------------------------------------", "label": "IndexError", "info": "dataset/ETHPy150Open swcarpentry/amy/workshops/views.py/object_changes"}
{"function": "@login_required\ndef profileupdaterequest_details(request, request_id):\n    update_request = get_object_or_404(ProfileUpdateRequest,\n                                       pk=request_id)\n\n    person_selected = False\n\n    person = None\n    form = None\n\n    # Nested lookup.\n    # First check if there's person with the same email, then maybe check if\n    # there's a person with the same first and last names.\n    try:\n        person = Person.objects.get(email=update_request.email)\n    except Person.DoesNotExist:\n        try:\n            person = Person.objects.get(personal=update_request.personal,\n                                        family=update_request.family)\n        except (Person.DoesNotExist, Person.MultipleObjectsReturned):\n            # Either none or multiple people with the same first and last\n            # names.\n            # But the user might have submitted some person by themselves. We\n            # should check that!\n            try:\n                form = PersonLookupForm(request.GET)\n                person = Person.objects.get(pk=int(request.GET['person_1']))\n                person_selected = True\n            except KeyError:\n                person = None\n                # if the form wasn't submitted, initialize it without any\n                # input data\n                form = PersonLookupForm()\n            except (__HOLE__, Person.DoesNotExist):\n                person = None\n\n    if person:\n        # check if the person has instructor badge\n        person.has_instructor_badge = Award.objects.filter(\n            badge__in=Badge.objects.instructor_badges(), person=person\n        ).exists()\n\n    try:\n        airport = Airport.objects.get(iata=update_request.airport_iata)\n    except Airport.DoesNotExist:\n        airport = None\n\n    context = {\n        'title': ('Instructor profile update request #{}'\n                  .format(update_request.pk)),\n        'new': update_request,\n        'old': person,\n        'person_form': form,\n        'person_selected': person_selected,\n        'form_helper': bootstrap_helper_get,\n        'airport': airport,\n    }\n    return render(request, 'workshops/profileupdaterequest.html', context)", "label": "ValueError", "info": "dataset/ETHPy150Open swcarpentry/amy/workshops/views.py/profileupdaterequest_details"}
{"function": "def gen_data(cluster, index_to_interpolation):\n    cluster.percents = {}\n    for element in cluster.all_elements:\n        try:\n            cluster.percents[index_to_interpolation[element]] += 1\n            a,b = index_to_interpolation[element]\n            cluster.percents[(b,a)] += 1\n        except __HOLE__:\n            cluster.percents[index_to_interpolation[element]] = 1\n            a,b = index_to_interpolation[element]\n            cluster.percents[(b,a)] = 1\n\n    for percent in cluster.percents:\n        cluster.percents[percent] /= 20.\n\n    data = {}\n    data[\"column names\"] = [str(x) for x in range(N)]\n    for i in range(0,N):\n        data[str(i)] = []\n        for j in range(0,N):\n            if (i,j) in cluster.percents:\n                data[str(i)].append( cluster.percents[(i,j)])\n            else:\n                data[str(i)].append(0)\n\n    return data", "label": "KeyError", "info": "dataset/ETHPy150Open victor-gil-sepulveda/pyProCT/validation/conformations/getMetaclusterPercents.py/gen_data"}
{"function": "def mkdir_p(self, path):\n        try:\n            os.makedirs(path)\n        except __HOLE__ as exc: # Python >2.5\n            if exc.errno == errno.EEXIST:\n                pass\n            else: raise", "label": "OSError", "info": "dataset/ETHPy150Open jimklo/LRSignature/src/LRSignature/tests/utils.py/Test.mkdir_p"}
{"function": "def set_value(self, pymux, cli, value):\n        \"\"\"\n        Take a string, and return an integer. Raise SetOptionError when the\n        given text does not parse to a positive integer.\n        \"\"\"\n        try:\n            value = int(value)\n            if value < 0:\n                raise ValueError\n        except __HOLE__:\n            raise SetOptionError('Expecting an integer.')\n        else:\n            setattr(pymux, self.attribute_name, value)", "label": "ValueError", "info": "dataset/ETHPy150Open jonathanslenders/pymux/pymux/options.py/PositiveIntOption.set_value"}
{"function": "def set_value(self, pymux, cli, value):\n        # Translate prefix to prompt_toolkit\n        try:\n            keys = pymux_key_to_prompt_toolkit_key_sequence(value)\n        except __HOLE__:\n            raise SetOptionError('Invalid key: %r' % (value, ))\n        else:\n            pymux.key_bindings_manager.prefix = keys", "label": "ValueError", "info": "dataset/ETHPy150Open jonathanslenders/pymux/pymux/options.py/KeyPrefixOption.set_value"}
{"function": "def set_value(self, pymux, cli, value):\n        try:\n            value = int(value)\n        except __HOLE__:\n            raise SetOptionError('Expecting an integer.')\n        else:\n            pymux.arrangement.base_index = value", "label": "ValueError", "info": "dataset/ETHPy150Open jonathanslenders/pymux/pymux/options.py/BaseIndexOption.set_value"}
{"function": "def _real_extract(self, url):\n        mobj = re.match(self._VALID_URL, url)\n        video_id = mobj.group('id')\n        if mobj.group('config'):\n            config_url = url + '&form=json'\n            config_url = config_url.replace('swf/', 'config/')\n            config_url = config_url.replace('onsite/', 'onsite/config/')\n            config = self._download_json(config_url, video_id, 'Downloading config')\n            smil_url = config['releaseUrl'] + '&format=SMIL&formats=MPEG4&manifest=f4m'\n        else:\n            smil_url = ('http://link.theplatform.com/s/dJ5BDC/{0}/meta.smil?'\n                        'format=smil&mbr=true'.format(video_id))\n\n        meta = self._download_xml(smil_url, video_id)\n        try:\n            error_msg = next(\n                n.attrib['abstract']\n                for n in meta.findall(_x('.//smil:ref'))\n                if n.attrib.get('title') == 'Geographic Restriction')\n        except __HOLE__:\n            pass\n        else:\n            raise ExtractorError(error_msg, expected=True)\n\n        info_url = 'http://link.theplatform.com/s/dJ5BDC/{0}?format=preview'.format(video_id)\n        info_json = self._download_webpage(info_url, video_id)\n        info = json.loads(info_json)\n\n        subtitles = {}\n        captions = info.get('captions')\n        if isinstance(captions, list):\n            for caption in captions:\n                lang, src = caption.get('lang'), caption.get('src')\n                if lang and src:\n                    subtitles[lang] = src\n\n        if self._downloader.params.get('listsubtitles', False):\n            self._list_available_subtitles(video_id, subtitles)\n            return\n\n        subtitles = self.extract_subtitles(video_id, subtitles)\n\n        head = meta.find(_x('smil:head'))\n        body = meta.find(_x('smil:body'))\n\n        f4m_node = body.find(_x('smil:seq//smil:video'))\n        if f4m_node is not None and '.f4m' in f4m_node.attrib['src']:\n            f4m_url = f4m_node.attrib['src']\n            if 'manifest.f4m?' not in f4m_url:\n                f4m_url += '?'\n            # the parameters are from syfy.com, other sites may use others,\n            # they also work for nbc.com\n            f4m_url += '&g=UXWGVKRWHFSP&hdcore=3.0.3'\n            formats = self._extract_f4m_formats(f4m_url, video_id)\n        else:\n            formats = []\n            switch = body.find(_x('smil:switch'))\n            if switch is not None:\n                base_url = head.find(_x('smil:meta')).attrib['base']\n                for f in switch.findall(_x('smil:video')):\n                    attr = f.attrib\n                    width = int(attr['width'])\n                    height = int(attr['height'])\n                    vbr = int(attr['system-bitrate']) // 1000\n                    format_id = '%dx%d_%dk' % (width, height, vbr)\n                    formats.append({\n                        'format_id': format_id,\n                        'url': base_url,\n                        'play_path': 'mp4:' + attr['src'],\n                        'ext': 'flv',\n                        'width': width,\n                        'height': height,\n                        'vbr': vbr,\n                    })\n            else:\n                switch = body.find(_x('smil:seq//smil:switch'))\n                for f in switch.findall(_x('smil:video')):\n                    attr = f.attrib\n                    vbr = int(attr['system-bitrate']) // 1000\n                    ext = determine_ext(attr['src'])\n                    if ext == 'once':\n                        ext = 'mp4'\n                    formats.append({\n                        'format_id': compat_str(vbr),\n                        'url': attr['src'],\n                        'vbr': vbr,\n                        'ext': ext,\n                    })\n            self._sort_formats(formats)\n\n        return {\n            'id': video_id,\n            'title': info['title'],\n            'subtitles': subtitles,\n            'formats': formats,\n            'description': info['description'],\n            'thumbnail': info['defaultThumbnailUrl'],\n            'duration': info['duration'] // 1000,\n        }", "label": "StopIteration", "info": "dataset/ETHPy150Open yasoob/youtube-dl-GUI/youtube_dl/extractor/theplatform.py/ThePlatformIE._real_extract"}
{"function": "def get_wallet_info(wallet):\n    \"\"\"\n    Add some info to a wallet object\n    \"\"\"\n    info = cache.get('walletinfo%d' % wallet.id)\n    if info == None:\n        try:\n            conn = connection.get_connection(wallet)\n            info = conn.getinfo()\n        except socket.error, v:  # pragma: no cover\n            if v[0] == errno.ECONNREFUSED:\n                info = False\n                flash(u\"%s: Connection refused\" % wallet.label, 'error')\n            elif v[0] == errno.ECONNRESET:\n                info = False\n                flash(u\"%s: Connection reset\" % wallet.label, 'error')\n            elif v[0] == errno.EHOSTUNREACH:\n                info = False\n                flash(u\"%s: Host unreachable\" % wallet.label, 'error')\n            elif v[0] == -2:  # WTF\n                info = False\n                flash(u\"%s: Unknown error\" % wallet.label, 'error')\n            #elif type(v) == socket.cachetime:\n            #    info = False\n            #    flash(u\"%s: cachetime\" % wallet.label, 'error')\n            else:\n                print v[0]\n                print dir(v)\n                print type(v)\n                flash(u\"Unhandled exception!\", 'error')\n        except __HOLE__:  # pragma: no cover\n            info = False\n            flash(u\"%s: Auth error (?)\" % wallet.label, 'error')\n        except BadStatusLine:  # pragma: no cover\n            # I think this was too much load on the node box\n            info = False\n            flash(u\"%s: Bad status line.\" % wallet.label, 'error')\n        except SSLError:  # pragma: no cover\n            info = False\n            flash(u\"%s: SSL error.\" % wallet.label, 'error')\n        except TransportException, e:  # pragma: no cover\n            if e.code == 403:\n                info = False\n                flash(u\"%s: \" % e.msg, 'error')\n            else:\n                flash(u\"Unhandled transport exception!\", 'error')\n        except Exception, e:  # pragma: no cover\n            print e\n            print dir(e)\n            print type(e)\n            flash(u\"Unhandled exception!\", 'error')\n    cache.set('walletinfo%d' % wallet.id, info, timeout=get_cachetime())\n    return info", "label": "ValueError", "info": "dataset/ETHPy150Open nkuttler/flaskwallet/walletapp/views/wallet.py/get_wallet_info"}
{"function": "def dispatch_request(self, id):\n        super(WalletGroupingsView, self).dispatch_request(id)\n        try:\n            groupings = self.conn.listaddressgroupings()\n        except __HOLE__:  # pragma: no cover\n            flash(u\"Your bitcoinrpc version doesn't support this function\",\n                  'error')\n            return redirect(url_for('wallet.wallet_detail', id=self.wallet.id))\n        return render_template(\"wallet/wallet/addressgroupings.html\", wallet=self.wallet,\n                               groupings=groupings)", "label": "AttributeError", "info": "dataset/ETHPy150Open nkuttler/flaskwallet/walletapp/views/wallet.py/WalletGroupingsView.dispatch_request"}
{"function": "def dispatch_request(self, id):\n        super(WalletUnlockedBase, self).dispatch_request(id)\n        try:\n            unlocked = self.conn.getinfo().unlocked_until\n            if not unlocked:\n                url = url_for('wallet.wallet_unlock', id=self.wallet.id)\n                flash('Wallet is currently locked. <a href=\"%s\">Unlock</a>' % url,\n                      \"warning\")\n        except __HOLE__:\n            #  Wallet is not encrypted\n            pass", "label": "AttributeError", "info": "dataset/ETHPy150Open nkuttler/flaskwallet/walletapp/views/wallet.py/WalletUnlockedBase.dispatch_request"}
{"function": "def __virtual__():\n    '''\n    Only work on systems which exclusively use sysvinit\n    '''\n    # Disable on these platforms, specific service modules exist:\n    disable = set((\n        'RedHat',\n        'CentOS',\n        'Amazon',\n        'ScientificLinux',\n        'CloudLinux',\n        'Fedora',\n        'Gentoo',\n        'Ubuntu',\n        'Debian',\n        'Devuan',\n        'Arch',\n        'Arch ARM',\n        'ALT',\n        'SUSE  Enterprise Server',\n        'SUSE',\n        'OEL',\n        'Linaro',\n        'elementary OS',\n        'McAfee  OS Server',\n        'Void',\n        'Mint',\n        'Raspbian'\n    ))\n    if __grains__.get('os', '') in disable:\n        return (False, 'Your OS is on the disabled list')\n    # Disable on all non-Linux OSes as well\n    if __grains__['kernel'] != 'Linux':\n        return (False, 'Non Linux OSes are not supported')\n    # Suse >=12.0 uses systemd\n    if __grains__.get('os_family', '') == 'Suse':\n        try:\n            # osrelease might be in decimal format (e.g. \"12.1\"), or for\n            # SLES might include service pack (e.g. \"11 SP3\"), so split on\n            # non-digit characters, and the zeroth element is the major\n            # number (it'd be so much simpler if it was always \"X.Y\"...)\n            import re\n            if int(re.split(r'\\D+', __grains__.get('osrelease', ''))[0]) >= 12:\n                return (False, 'Suse version greater than or equal to 12 is not supported')\n        except __HOLE__:\n            return (False, 'You are missing the os_family grain')\n    return 'service'", "label": "ValueError", "info": "dataset/ETHPy150Open saltstack/salt/salt/modules/service.py/__virtual__"}
{"function": "def get_zone(self, zone_id):\n        # TODO: This is not efficient\n        zones = self.list_zones()\n\n        try:\n            zone = [z for z in zones if z.id == zone_id][0]\n        except __HOLE__:\n            raise ZoneDoesNotExistError(value='', driver=self, zone_id=zone_id)\n\n        return zone", "label": "IndexError", "info": "dataset/ETHPy150Open apache/libcloud/libcloud/dns/drivers/cloudflare.py/CloudFlareDNSDriver.get_zone"}
{"function": "def test_import(self):\n        \"\"\"\n        Check that we can import the library\n        \"\"\"\n        try:\n            import octavo\n        except __HOLE__:\n            self.fail(\"Could not import the octavo library.\")", "label": "ImportError", "info": "dataset/ETHPy150Open DistrictDataLabs/science-bookclub/tests/__init__.py/InitializationTest.test_import"}
{"function": "def LoadPretrained(self, param):\n    node1_name = param.pretrained_model_node1\n    node2_name = param.pretrained_model_node2\n    if node1_name == '':\n      node1_name = self.proto.node1\n    if node2_name == '':\n      node2_name = self.proto.node2\n\n    if param.transpose_pretrained:\n      temp = node1_name\n      node1_name = node2_name\n      node2_name = temp\n    mat = None\n    for pretrained_model in param.pretrained_model:\n      model_file = os.path.join(self.prefix, pretrained_model)\n      ext = os.path.splitext(pretrained_model)[1]\n      if ext == '.npz':\n        npzfile = np.load(model_file)\n        if param.name == 'bias':\n          this_mat = np.nan_to_num(npzfile['mean'] / npzfile['std'])\n        elif param.name == 'precision':\n          this_mat = np.nan_to_num(1. / npzfile['std'])\n      elif ext == '.npy':\n        this_mat = np.load(model_file)\n      else:\n        model = util.ReadModel(model_file)\n        try:\n          edge = next(e for e in model.edge if e.node1 == node1_name and e.node2 == node2_name)\n        except __HOLE__ as e:\n          print 'No edge found between %s and %s in model %s.' % (node1_name, node2_name, model_file)\n          raise e\n        pretrained_param = next(p for p in edge.param if p.name == param.name)\n        assert pretrained_param.mat != '',\\\n                'Pretrained param %s in edge %s:%s of model %s is empty!!' % (\n                  pretrained_param.name, edge.node1, edge.node2, pretrained_model)\n        if param.transpose_pretrained:\n          assert param.dimensions == pretrained_param.dimensions[::-1],\\\n              'Param has shape %s but transposed pretrained param has shape %s' % (\n                param.dimensions, reversed(pretrained_param.dimensions))\n        else:\n          assert param.dimensions == pretrained_param.dimensions,\\\n              'Param has shape %s but pretrained param has shape %s' % (\n                param.dimensions, pretrained_param.dimensions)\n        this_mat = param.mult_factor * util.ParameterAsNumpy(pretrained_param)\n      if param.transpose_pretrained:\n        this_mat = this_mat.T\n      if mat is None:\n        mat = this_mat\n      else:\n        mat += this_mat\n    return mat / len(param.pretrained_model)", "label": "StopIteration", "info": "dataset/ETHPy150Open nitishsrivastava/deepnet/deepnet/edge.py/Edge.LoadPretrained"}
{"function": "def test():\n    os.environ['CONF'] = sample('b.yaml')\n    sys.argv = sys.argv[0:1] + ['--conf-mysql', 'host: localhost', '--conf', sample('a.yaml'),\n                                '--log-level', 'WARN']\n    try:\n        main()\n    except __HOLE__:\n        pass", "label": "SystemExit", "info": "dataset/ETHPy150Open EverythingMe/click-config/click_config/test/test_wrapper.py/test"}
{"function": "def test_listIdentifiers_from_none(self):\n        # test listIdentifiers with until argument as None explicitly\n\n        # XXX unfortunately a white box test relying on particular\n        # exception behavior of the fake server. We do verify whether\n        # from or from_ doesn't appear in the request args though\n        try:\n            headers = fakeclient.listIdentifiers(from_=None,\n                                                 metadataPrefix='oai_dc')\n        except __HOLE__, e:\n            self.assertEquals('metadataPrefix=oai_dc&verb=ListIdentifiers',\n                              e.args[0])", "label": "KeyError", "info": "dataset/ETHPy150Open infrae/pyoai/src/oaipmh/tests/test_client.py/ClientTestCase.test_listIdentifiers_from_none"}
{"function": "def _ExecuteBackgroundThreadTasks(worker_id, task_queue, response_queue):\n  \"\"\"Executes tasks received on a task queue.\n\n  Executed in a child Thread by _BackgroundThreadTaskManager.\n\n  Args:\n    worker_id: int. Identifier for the child thread relative to other child\n        threads.\n    task_queue: _NonPollingSingleReaderQueue. Queue from which input is read.\n        Each value in the queue can be one of three types of values. If it is a\n        (task_id, _BackgroundTask) pair, the task is executed on this thread.\n        If it is _THREAD_STOP_PROCESSING, the thread stops executing. If it is\n        _THREAD_WAIT_FOR_KEYBOARD_INTERRUPT, the thread waits for a\n        KeyboardInterrupt.\n    response_queue: _SingleReaderQueue. Queue to which output is written. It\n        receives worker_id when this thread's bootstrap code has completed and\n        receives a (worker_id, task_id) pair for each task completed on this\n        thread.\n  \"\"\"\n  try:\n    response_queue.Put(worker_id)\n    while True:\n      task_tuple = task_queue.Get()\n      if task_tuple == _THREAD_STOP_PROCESSING:\n        break\n      elif task_tuple == _THREAD_WAIT_FOR_KEYBOARD_INTERRUPT:\n        while True:\n          time.sleep(_WAIT_MAX_RECHECK_DELAY)\n      task_id, task = task_tuple\n      task.Run()\n      response_queue.Put((worker_id, task_id))\n  except __HOLE__:\n    # TODO(skschneider): Detect when the log would be unhelpful (e.g. if the\n    # current thread was spinning in the _THREAD_WAIT_FOR_KEYBOARD_INTERRUPT\n    # sub-loop). Only log in helpful cases, like when the task is interrupted.\n    logging.debug('Child thread %s received a KeyboardInterrupt from its '\n                  'parent.', worker_id, exc_info=True)", "label": "KeyboardInterrupt", "info": "dataset/ETHPy150Open GoogleCloudPlatform/PerfKitBenchmarker/perfkitbenchmarker/background_tasks.py/_ExecuteBackgroundThreadTasks"}
{"function": "def _RunParallelTasks(target_arg_tuples, max_concurrency, get_task_manager,\n                      parallel_exception_class):\n  \"\"\"Executes function calls concurrently in separate threads or processes.\n\n  Args:\n    target_arg_tuples: list of (target, args, kwargs) tuples. Each tuple\n        contains the function to call and the arguments to pass it.\n    max_concurrency: int or None. The maximum number of concurrent new\n        threads or processes.\n    get_task_manager: Callable that accepts an int max_concurrency arg and\n        returns a _TaskManager.\n    parallel_exception_class: Type of exception to raise upon an exception in\n        one of the called functions.\n\n  Returns:\n    list of function return values in the order corresponding to the order of\n    target_arg_tuples.\n\n  Raises:\n    parallel_exception_class: When an exception occurred in any of the called\n        functions.\n  \"\"\"\n  thread_context = _BackgroundTaskThreadContext()\n  max_concurrency = min(max_concurrency, len(target_arg_tuples))\n  error_strings = []\n  started_task_count = 0\n  active_task_count = 0\n  with get_task_manager(max_concurrency) as task_manager:\n    try:\n      while started_task_count < len(target_arg_tuples) or active_task_count:\n        if (started_task_count < len(target_arg_tuples) and\n            active_task_count < max_concurrency):\n          # Start a new task.\n          target, args, kwargs = target_arg_tuples[started_task_count]\n          task_manager.StartTask(target, args, kwargs, thread_context)\n          started_task_count += 1\n          active_task_count += 1\n          continue\n\n        # Wait for a task to complete.\n        task_id = task_manager.AwaitAnyTask()\n        active_task_count -= 1\n        # If the task failed, it may still be a long time until all remaining\n        # tasks complete. Log the failure immediately before continuing to wait\n        # for other tasks.\n        stacktrace = task_manager.tasks[task_id].traceback\n        if stacktrace:\n          msg = ('Exception occurred while calling {0}:{1}{2}'.format(\n              _GetCallString(target_arg_tuples[task_id]), os.linesep,\n              stacktrace))\n          logging.error(msg)\n          error_strings.append(msg)\n\n    except __HOLE__:\n      logging.error(\n          'Received KeyboardInterrupt while executing parallel tasks. Waiting '\n          'for %s tasks to clean up.', active_task_count)\n      task_manager.HandleKeyboardInterrupt()\n      raise\n\n  if error_strings:\n    # TODO(skschneider): Combine errors.VmUtil.ThreadException and\n    # errors.VmUtil.CalledProcessException so this can be a single exception\n    # type.\n    raise parallel_exception_class(\n        'The following exceptions occurred during parallel execution:'\n        '{0}{1}'.format(os.linesep, os.linesep.join(error_strings)))\n  results = [task.return_value for task in task_manager.tasks]\n  assert len(target_arg_tuples) == len(results), (target_arg_tuples, results)\n  return results", "label": "KeyboardInterrupt", "info": "dataset/ETHPy150Open GoogleCloudPlatform/PerfKitBenchmarker/perfkitbenchmarker/background_tasks.py/_RunParallelTasks"}
{"function": "def show_timezone_quickpanel(callback, selected_item):\n    global s\n    show_quick_panel = sublime.active_window().show_quick_panel\n    if ST2:\n        show_quick_panel(pytz.all_timezones, callback)\n    else:\n        try:\n            selected_index = pytz.all_timezones.index(selected_item)\n        except __HOLE__:\n            selected_index = 0\n        show_quick_panel(pytz.all_timezones, callback,\n                         selected_index=selected_index)\n\n# I wrote this for InactivePanes, but why not just use it here as well?\n# TODO write methods to change settings and flush changes.", "label": "ValueError", "info": "dataset/ETHPy150Open FichteFoll/InsertDate/insert_date.py/show_timezone_quickpanel"}
{"function": "def __init__(self, parent):\n    parent.title = \"LungRegistration\" # TODO make this more human readable by adding spaces\n    parent.categories = [\"Chest Imaging Platform\"]\n    parent.dependencies = []\n    parent.contributors = [\"Applied Chest Imaging Laboratory, Brigham and Women's Hopsital\"] # replace with \"Firstname Lastname (Org)\"\n    parent.helpText = \"\"\"\n    Simple Lung registration module\n    \"\"\"\n    parent.acknowledgementText = \"\"\"\n        This work is funded by the National Heart, Lung, And Blood Institute of the National Institutes of Health under Award Number R01HL116931. The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health.\n\"\"\" # replace with organization, grant and thanks.\n    self.parent = parent\n\n    # Add this test to the SelfTest module's list for discovery when the module\n    # is created.  Since this module may be discovered before SelfTests itself,\n    # create the list if it doesn't already exist.\n    try:\n      slicer.selfTests\n    except __HOLE__:\n      slicer.selfTests = {}\n    slicer.selfTests['LungRegistration'] = self.runTest", "label": "AttributeError", "info": "dataset/ETHPy150Open acil-bwh/SlicerCIP/Scripted/LungRegistration/LungRegistration.py/LungRegistration.__init__"}
{"function": "def onReload(self,moduleName=\"LungRegistration\"):\n    \"\"\"Generic reload method for any scripted module.\n    ModuleWizard will subsitute correct default moduleName.\n    \"\"\"\n    import imp, sys, os, slicer\n\n    widgetName = moduleName + \"Widget\"\n\n    # reload the source code\n    # - set source file path\n    # - load the module to the global space\n    filePath = eval('slicer.modules.%s.path' % moduleName.lower())\n    p = os.path.dirname(filePath)\n    if not sys.path.__contains__(p):\n      sys.path.insert(0,p)\n    fp = open(filePath, \"r\")\n    globals()[moduleName] = imp.load_module(\n        moduleName, fp, filePath, ('.py', 'r', imp.PY_SOURCE))\n    fp.close()\n\n    # rebuild the widget\n    # - find and hide the existing widget\n    # - create a new widget in the existing parent\n    parent = slicer.util.findChildren(name='%s Reload' % moduleName)[0].parent().parent()\n    for child in parent.children():\n      try:\n        child.hide()\n      except __HOLE__:\n        pass\n    # Remove spacer items\n    item = parent.layout().itemAt(0)\n    while item:\n      parent.layout().removeItem(item)\n      item = parent.layout().itemAt(0)\n\n    # delete the old widget instance\n    if hasattr(globals()['slicer'].modules, widgetName):\n      getattr(globals()['slicer'].modules, widgetName).cleanup()\n\n    # create new widget inside existing parent\n    globals()[widgetName.lower()] = eval(\n        'globals()[\"%s\"].%s(parent)' % (moduleName, widgetName))\n    globals()[widgetName.lower()].setup()\n    setattr(globals()['slicer'].modules, widgetName, globals()[widgetName.lower()])", "label": "AttributeError", "info": "dataset/ETHPy150Open acil-bwh/SlicerCIP/Scripted/LungRegistration/LungRegistration.py/LungRegistrationWidget.onReload"}
{"function": "def validateArrayValues(arrayList, valuesMustBePositive):\n\tfor i in range(len(arrayList)):\n\t\ttry:\n\t\t\tarrayList[i] = eval(arrayList[i])\n\t\texcept (__HOLE__, SyntaxError):\n\t\t\treturn\n\t\tif valuesMustBePositive:\n\t\t\tif arrayList[i] < 0:\n\t\t\t\treturn\n\treturn arrayList", "label": "NameError", "info": "dataset/ETHPy150Open adobe-type-tools/python-scripts/UFOInstanceGenerator.py/validateArrayValues"}
{"function": "def readInstanceFile(instancesFilePath):\n\tf = open(instancesFilePath, \"rt\")\n\tdata = f.read()\n\tf.close()\n\t\n\tlines = data.splitlines()\n\t\n\ti = 0\n\tparseError = 0\n\tkeyDict = copy.copy(kFixedFieldKeys)\n\tnumKeys = kNumFixedFields\n\tnumLines = len(lines)\n\tinstancesList = []\n\t\n\tfor i in range(numLines):\n\t\tline = lines[i]\n\t\t\n\t\t# Skip over blank lines\n\t\tline2 = line.strip()\n\t\tif not line2:\n\t\t\tcontinue\n\n\t\t# Get rid of all comments. If we find a key definition comment line, parse it.\n\t\tcommentIndex = line.find('#')\n\t\tif commentIndex >= 0:\n\t\t\tif line.startswith(kFieldsKey):\n\t\t\t\tif instancesList:\n\t\t\t\t\tprint >> sys.stderr, \"ERROR: Header line (%s) must preceed a data line.\" % kFieldsKey\n\t\t\t\t\traise\n\t\t\t\t# parse the line with the field names.\n\t\t\t\tline = line[len(kFieldsKey):]\n\t\t\t\tline = line.strip()\n\t\t\t\tkeys = line.split('\\t')\n\t\t\t\tkeys = map(lambda name: name.strip(), keys)\n\t\t\t\tnumKeys = len(keys)\n\t\t\t\tk = kNumFixedFields\n\t\t\t\twhile k < numKeys:\n\t\t\t\t\tkeyDict[k] = keys[k]\n\t\t\t\t\tk +=1\n\t\t\t\tcontinue\n\t\t\telse:\n\t\t\t\tline = line[:commentIndex]\n\t\t\t\tcontinue\n\n\t\t# Must be a data line.\n\t\tfields = line.split('\\t')\n\t\tfields = map(lambda datum: datum.strip(), fields)\n\t\tnumFields = len(fields)\n\t\tif (numFields != numKeys):\n\t\t\tprint >> sys.stderr, \"ERROR: In line %s, the number of fields %s does not match the number of key names %s (FamilyName, FontName, FullName, Weight, Coords, IsBold).\" % (i+1, numFields, numKeys)\n\t\t\tparseError = 1\n\t\t\tcontinue\n\t\t\n\t\tinstanceDict= {}\n\t\t#Build a dict from key to value. Some kinds of values needs special processing.\n\t\tfor k in range(numFields):\n\t\t\tkey = keyDict[k]\n\t\t\tfield = fields[k]\n\t\t\tif not field:\n\t\t\t\tcontinue\n\t\t\tif field in [\"Default\", \"None\", \"FontBBox\"]:\n\t\t\t\t# FontBBox is no longer supported - I calculate the real\n\t\t\t\t# instance fontBBox from the glyph metrics instead,\n\t\t\t\tcontinue\n\t\t\tif key == kFontName:\n\t\t\t\tvalue = field\n\t\t\tif key == kMasters:\n\t\t\t\tvalue = eval(field)\n\t\t\telif key in [kExtraGlyphs, kExceptionSuffixes]:\n\t\t\t\tvalue = eval(field)\n\t\t\telif key in [kIsBoldKey, kIsItalicKey, kCoordsKey]:\n\t\t\t\ttry:\n\t\t\t\t\tvalue = eval(field) # this works for all three fields.\n\t\t\t\t\t\n\t\t\t\t\tif key == kIsBoldKey: # need to convert to Type 1 field key.\n\t\t\t\t\t\tinstanceDict[key] = value\n\t\t\t\t\t\t# add kForceBold key.\n\t\t\t\t\t\tkey = kForceBold\n\t\t\t\t\t\tif value == 1:\n\t\t\t\t\t\t\tvalue = \"true\"\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tvalue = \"false\"\n\t\t\t\t\telif key == kIsItalicKey:\n\t\t\t\t\t\tif value == 1:\n\t\t\t\t\t\t\tvalue = \"true\"\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tvalue = \"false\"\n\t\t\t\t\telif key == kCoordsKey:\n\t\t\t\t\t\tif type(value) == type(0):\n\t\t\t\t\t\t\tvalue = (value,)\n\t\t\t\texcept (__HOLE__, SyntaxError):\n\t\t\t\t\tprint >> sys.stderr, \"ERROR: In line %s, the %s field has an invalid value.\" % (i+1, key)\n\t\t\t\t\tparseError = 1\n\t\t\t\t\tcontinue\n\n\t\t\telif field[0] in [\"[\",\"{\"]: # it is a Type 1 array value. Turn it into a list and verify that there's an even number of values for the alignment zones\n\t\t\t\tvalue = field[1:-1].split() # Remove the begin and end brackets/braces, and make a list\n\t\t\t\t\n\t\t\t\tif key in kAlignmentZonesKeys:\n\t\t\t\t\tif len(value) % 2 != 0:\n\t\t\t\t\t\tprint >> sys.stderr, \"ERROR: In line %s, the %s field does not have an even number of values.\" % (i+1, key)\n\t\t\t\t\t\tparseError = 1\n\t\t\t\t\t\tcontinue\n\t\t\t\t\n\t\t\t\tif key in kTopAlignZonesKeys: # The Type 1 spec only allows 7 top zones (7 pairs of values)\n\t\t\t\t\tif len(value) > kMaxTopZonesSize:\n\t\t\t\t\t\tprint >> sys.stderr, \"ERROR: In line %s, the %s field has more than %d values.\" % (i+1, key, kMaxTopZonesSize)\n\t\t\t\t\t\tparseError = 1\n\t\t\t\t\t\tcontinue\n\t\t\t\t\telse:\n\t\t\t\t\t\tnewArray = validateArrayValues(value, False) # False = values do NOT have to be all positive\n\t\t\t\t\t\tif newArray:\n\t\t\t\t\t\t\tvalue = newArray\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tprint >> sys.stderr, \"ERROR: In line %s, the %s field contains invalid values.\" % (i+1, key)\n\t\t\t\t\t\t\tparseError = 1\n\t\t\t\t\t\t\tcontinue\n\t\t\t\t\tcurrentArray = value[:] # make copy, not reference\n\t\t\t\t\tvalue.sort()\n\t\t\t\t\tif currentArray != value:\n\t\t\t\t\t\tprint \"WARNING: In line %s, the values in the %s field were sorted in ascending order.\" % (i+1, key)\n\t\t\t\t\n\t\t\t\tif key in kBotAlignZonesKeys: # The Type 1 spec only allows 5 top zones (5 pairs of values)\n\t\t\t\t\tif len(value) > kMaxBotZonesSize:\n\t\t\t\t\t\tprint >> sys.stderr, \"ERROR: In line %s, the %s field has more than %d values.\" % (i+1, key, kMaxBotZonesSize)\n\t\t\t\t\t\tparseError = 1\n\t\t\t\t\t\tcontinue\n\t\t\t\t\telse:\n\t\t\t\t\t\tnewArray = validateArrayValues(value, False) # False = values do NOT have to be all positive\n\t\t\t\t\t\tif newArray:\n\t\t\t\t\t\t\tvalue = newArray\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tprint >> sys.stderr, \"ERROR: In line %s, the %s field contains invalid values.\" % (i+1, key)\n\t\t\t\t\t\t\tparseError = 1\n\t\t\t\t\t\t\tcontinue\n\t\t\t\t\tcurrentArray = value[:] # make copy, not reference\n\t\t\t\t\tvalue.sort()\n\t\t\t\t\tif currentArray != value:\n\t\t\t\t\t\tprint \"WARNING: In line %s, the values in the %s field were sorted in ascending order.\" % (i+1, key)\n\t\t\t\t\n\t\t\t\tif key in kStdStemsKeys:\n\t\t\t\t\tif len(value) > kMaxStdStemsSize:\n\t\t\t\t\t\tprint >> sys.stderr, \"ERROR: In line %s, the %s field can only have %d value.\" % (i+1, key, kMaxStdStemsSize)\n\t\t\t\t\t\tparseError = 1\n\t\t\t\t\t\tcontinue\n\t\t\t\t\telse:\n\t\t\t\t\t\tnewArray = validateArrayValues(value, True) # True = all values must be positive\n\t\t\t\t\t\tif newArray:\n\t\t\t\t\t\t\tvalue = newArray\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tprint >> sys.stderr, \"ERROR: In line %s, the %s field has an invalid value.\" % (i+1, key)\n\t\t\t\t\t\t\tparseError = 1\n\t\t\t\t\t\t\tcontinue\n\t\t\t\t\n\t\t\t\tif key in kStemSnapKeys: # The Type 1 spec only allows 12 stem widths, including 1 standard stem\n\t\t\t\t\tif len(value) > kMaxStemSnapSize:\n\t\t\t\t\t\tprint >> sys.stderr, \"ERROR: In line %s, the %s field has more than %d values.\" % (i+1, key, kMaxStemSnapSize)\n\t\t\t\t\t\tparseError = 1\n\t\t\t\t\t\tcontinue\n\t\t\t\t\telse:\n\t\t\t\t\t\tnewArray = validateArrayValues(value, True) # True = all values must be positive\n\t\t\t\t\t\tif newArray:\n\t\t\t\t\t\t\tvalue = newArray\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tprint >> sys.stderr, \"ERROR: In line %s, the %s field contains invalid values.\" % (i+1, key)\n\t\t\t\t\t\t\tparseError = 1\n\t\t\t\t\t\t\tcontinue\n\t\t\t\t\tcurrentArray = value[:] # make copy, not reference\n\t\t\t\t\tvalue.sort()\n\t\t\t\t\tif currentArray != value:\n\t\t\t\t\t\tprint \"WARNING: In line %s, the values in the %s field were sorted in ascending order.\" % (i+1, key)\n\t\t\telse:\n\t\t\t\t# either a single number or a string.\n\t\t\t\tif re.match(r\"^[-.\\d]+$\", field):\n\t\t\t\t\tvalue = field #it is a Type 1 number. Pass as is, as a string.\n\t\t\t\telse:\n\t\t\t\t\tvalue = field\n\t\t\t\n\t\t\tinstanceDict[key] = value\n\t\t\t\t\n\t\tif (kStdHW in instanceDict and kStemSnapH not in instanceDict) or (kStdHW not in instanceDict and kStemSnapH in instanceDict):\n\t\t\tprint >> sys.stderr, \"ERROR: In line %s, either the %s value or the %s values are missing or were invalid.\" % (i+1, kStdHW, kStemSnapH)\n\t\t\tparseError = 1\n\t\telif (kStdHW in instanceDict and kStemSnapH in instanceDict): # cannot be just 'else' because it will generate a 'KeyError' when these hinting parameters are not provided in the 'instances' file\n\t\t\tif instanceDict[kStemSnapH][0] != instanceDict[kStdHW][0]:\n\t\t\t\tprint >> sys.stderr, \"ERROR: In line %s, the first value in %s must be the same as the %s value.\" % (i+1, kStemSnapH, kStdHW)\n\t\t\t\tparseError = 1\n\n\t\tif (kStdVW in instanceDict and kStemSnapV not in instanceDict) or (kStdVW not in instanceDict and kStemSnapV in instanceDict):\n\t\t\tprint >> sys.stderr, \"ERROR: In line %s, either the %s value or the %s values are missing or were invalid.\" % (i+1, kStdVW, kStemSnapV)\n\t\t\tparseError = 1\n\t\telif (kStdVW in instanceDict and kStemSnapV in instanceDict): # cannot be just 'else' because it will generate a 'KeyError' when these hinting parameters are not provided in the 'instances' file\n\t\t\tif instanceDict[kStemSnapV][0] != instanceDict[kStdVW][0]:\n\t\t\t\tprint >> sys.stderr, \"ERROR: In line %s, the first value in %s must be the same as the %s value.\" % (i+1, kStemSnapV, kStdVW)\n\t\t\t\tparseError = 1\n\t\t\n\t\tinstancesList.append(instanceDict)\n\t\t\n\tif parseError or len(instancesList) == 0:\n\t\traise\n\t\t\n\treturn instancesList", "label": "NameError", "info": "dataset/ETHPy150Open adobe-type-tools/python-scripts/UFOInstanceGenerator.py/readInstanceFile"}
{"function": "def makeInstance(counter, ufoMasters, instanceInfo, outputDirPath, options):\n\n\tif len(ufoMasters) == 2:\n\t\t'Linear interpolation with 2 masters'\n\t\tpass\n\telse:\n\t\t'Linear interpolation with intermediate masters'\n\t\tufoMasters = [master for master in ufoMasters if os.path.basename(master.path) in instanceInfo.get(kMasters)]\n\n\ttry:\n\t\tfaceName = instanceInfo[kFontName].split('-')[1]\n\texcept __HOLE__:\n\t\tfaceName = 'Regular'\n\n\tprint\n\tprint \"%s (%d/%d)\" % (faceName, counter[0], counter[1])\n\t\n\t# Calculate the value of the interpolation factor\n\t# XXX It's currently assuming a 0-1000 axis\n\tinterpolationFactor = instanceInfo[kCoordsKey][0]/1000.000\n\n\tglyphOrder = ufoMasters[0].lib[\"public.glyphOrder\"]\n\t\n\t# aGlyph.isCompatible(otherGlyph, report=True)\n# \tfor glyphName in glyphOrder:\n# \t\tufoMasters[0][glyphName].isCompatible(ufoMasters[1][glyphName], True)\n\t\n\tufoInstance = NewFont()\n\t\n\t# Interpolate the masters\n\t# Documentation: http://www.robofab.org/howto/interpolate.html\n\t# aFont.interpolate(factor, minFont, maxFont, suppressError=True, analyzeOnly=False)\n\t# aFont.interpolate() interpolates:\n\t#\t- positions of components\n\t#\t- anchors\n\t#\t- ascender\n\t#\t- descender\n\t#\t- glyph widths for the whole font\n\tufoInstance.interpolate(interpolationFactor, ufoMasters[0], ufoMasters[1])\n\t\n\t# Round all the point coordinates to whole integer numbers\n\tufoInstance.round()\n\n\t# Interpolate the kerning\n\t# Documentation: http://www.robofab.org/objects/kerning.html\n\t# f.kerning.interpolate(sourceDictOne, sourceDictTwo, value, clearExisting=True)\n\tif len(ufoMasters[0].kerning):\n\t\tufoInstance.kerning.interpolate(ufoMasters[0].kerning, ufoMasters[1].kerning, interpolationFactor)\n\t\tufoInstance.kerning.round(1) # convert the interpolated values to integers\n\t\n\tfor glyphName in glyphOrder:\n\t\tufoInstance[glyphName].unicode = ufoMasters[0][glyphName].unicode\n\n\t\tif len(ufoMasters[0][glyphName]) != len(ufoInstance[glyphName]):\n\t\t\tprint \"\\tWARNING: Interpolation failed in glyph %s\" % glyphName\n\n\tstyleName = instanceInfo[kFullName].replace(instanceInfo[kFamilyName], '').strip()\n\tufoInstance.info.styleName = styleName\n\n\tufoInstance.info.familyName = instanceInfo[kFamilyName]\n\tufoInstance.info.postscriptFontName = instanceInfo[kFontName]\n\tufoInstance.info.postscriptFullName = instanceInfo[kFullName]\n\tufoInstance.info.postscriptWeightName = instanceInfo[kWeight]\n\tufoInstance.info.postscriptForceBold = True if instanceInfo[kIsBoldKey] else False\n\t\n\tufoInstance.lib = ufoMasters[0].lib\n\tufoInstance.groups = ufoMasters[0].groups\n\t\n\tufoInstance.info.copyright = ufoMasters[0].info.copyright\n\tufoInstance.info.trademark = ufoMasters[0].info.trademark\n\tufoInstance.info.unitsPerEm = ufoMasters[0].info.unitsPerEm\n\tufoInstance.info.versionMajor = ufoMasters[0].info.versionMajor\n\tufoInstance.info.versionMinor = ufoMasters[0].info.versionMinor\n\tufoInstance.info.postscriptIsFixedPitch = ufoMasters[0].info.postscriptIsFixedPitch\n\t\n\t# ascender\n\tif ufoMasters[0].info.ascender and ufoMasters[1].info.ascender:\n\t\tufoInstance.info.ascender = int(round(objectsBase._interpolate(ufoMasters[0].info.ascender, ufoMasters[1].info.ascender, interpolationFactor)))\n\t# descender\n\tif ufoMasters[0].info.descender and ufoMasters[1].info.descender:\n\t\tufoInstance.info.descender = int(round(objectsBase._interpolate(ufoMasters[0].info.descender, ufoMasters[1].info.descender, interpolationFactor)))\n\t# capHeight\n\tif ufoMasters[0].info.capHeight and ufoMasters[1].info.capHeight:\n\t\tufoInstance.info.capHeight = int(round(objectsBase._interpolate(ufoMasters[0].info.capHeight, ufoMasters[1].info.capHeight, interpolationFactor)))\n\t# xHeight\n\tif ufoMasters[0].info.xHeight and ufoMasters[1].info.xHeight:\n\t\tufoInstance.info.xHeight = int(round(objectsBase._interpolate(ufoMasters[0].info.xHeight, ufoMasters[1].info.xHeight, interpolationFactor)))\n\t# italicAngle\n\tif (ufoMasters[0].info.italicAngle != None) and (ufoMasters[1].info.italicAngle != None):\n\t\tufoInstance.info.italicAngle = int(round(objectsBase._interpolate(ufoMasters[0].info.italicAngle, ufoMasters[1].info.italicAngle, interpolationFactor)))\n\t# postscriptUnderlinePosition\n\tif ufoMasters[0].info.postscriptUnderlinePosition and ufoMasters[1].info.postscriptUnderlinePosition:\n\t\tufoInstance.info.postscriptUnderlinePosition = int(round(objectsBase._interpolate(ufoMasters[0].info.postscriptUnderlinePosition, ufoMasters[1].info.postscriptUnderlinePosition, interpolationFactor)))\n\t# postscriptUnderlineThickness\n\tif ufoMasters[0].info.postscriptUnderlineThickness and ufoMasters[1].info.postscriptUnderlineThickness:\n\t\tufoInstance.info.postscriptUnderlineThickness = int(round(objectsBase._interpolate(ufoMasters[0].info.postscriptUnderlineThickness, ufoMasters[1].info.postscriptUnderlineThickness, interpolationFactor)))\n\t# postscriptBlueFuzz\n\tif (ufoMasters[0].info.postscriptBlueFuzz != None) and (ufoMasters[1].info.postscriptBlueFuzz != None):\n\t\tufoInstance.info.postscriptBlueFuzz = int(round(objectsBase._interpolate(ufoMasters[0].info.postscriptBlueFuzz, ufoMasters[1].info.postscriptBlueFuzz, interpolationFactor)))\n\t# postscriptBlueScale\n\tif ufoMasters[0].info.postscriptBlueScale and ufoMasters[1].info.postscriptBlueScale:\n\t\tufoInstance.info.postscriptBlueScale = objectsBase._interpolate(ufoMasters[0].info.postscriptBlueScale, ufoMasters[1].info.postscriptBlueScale, interpolationFactor)\n\t# postscriptBlueShift\n\tif ufoMasters[0].info.postscriptBlueShift and ufoMasters[1].info.postscriptBlueShift:\n\t\tufoInstance.info.postscriptBlueShift = int(round(objectsBase._interpolate(ufoMasters[0].info.postscriptBlueShift, ufoMasters[1].info.postscriptBlueShift, interpolationFactor)))\n\n\t# postscriptBlueValues\n\tif len(ufoMasters[0].info.postscriptBlueValues) == len(ufoMasters[1].info.postscriptBlueValues):\n\t\tufoMasters[0].info.postscriptBlueValues.sort()\n\t\tufoMasters[1].info.postscriptBlueValues.sort()\n\t\ttempArray = []\n\t\tfor i in range(len(ufoMasters[0].info.postscriptBlueValues)):\n\t\t\ttempArray.append(int(round(objectsBase._interpolate(ufoMasters[0].info.postscriptBlueValues[i], ufoMasters[1].info.postscriptBlueValues[i], interpolationFactor))))\n\t\tufoInstance.info.postscriptBlueValues = tempArray\n\t# postscriptOtherBlues\n\tif len(ufoMasters[0].info.postscriptOtherBlues) == len(ufoMasters[1].info.postscriptOtherBlues):\n\t\tufoMasters[0].info.postscriptOtherBlues.sort()\n\t\tufoMasters[1].info.postscriptOtherBlues.sort()\n\t\ttempArray = []\n\t\tfor i in range(len(ufoMasters[0].info.postscriptOtherBlues)):\n\t\t\ttempArray.append(int(round(objectsBase._interpolate(ufoMasters[0].info.postscriptOtherBlues[i], ufoMasters[1].info.postscriptOtherBlues[i], interpolationFactor))))\n\t\tufoInstance.info.postscriptOtherBlues = tempArray\n\t# postscriptFamilyBlues\n\tif len(ufoMasters[0].info.postscriptFamilyBlues) == len(ufoMasters[1].info.postscriptFamilyBlues):\n\t\tufoMasters[0].info.postscriptFamilyBlues.sort()\n\t\tufoMasters[1].info.postscriptFamilyBlues.sort()\n\t\ttempArray = []\n\t\tfor i in range(len(ufoMasters[0].info.postscriptFamilyBlues)):\n\t\t\ttempArray.append(int(round(objectsBase._interpolate(ufoMasters[0].info.postscriptFamilyBlues[i], ufoMasters[1].info.postscriptFamilyBlues[i], interpolationFactor))))\n\t\tufoInstance.info.postscriptFamilyBlues = tempArray\n\t# postscriptFamilyOtherBlues\n\tif len(ufoMasters[0].info.postscriptFamilyOtherBlues) == len(ufoMasters[1].info.postscriptFamilyOtherBlues):\n\t\tufoMasters[0].info.postscriptFamilyOtherBlues.sort()\n\t\tufoMasters[1].info.postscriptFamilyOtherBlues.sort()\n\t\ttempArray = []\n\t\tfor i in range(len(ufoMasters[0].info.postscriptFamilyOtherBlues)):\n\t\t\ttempArray.append(int(round(objectsBase._interpolate(ufoMasters[0].info.postscriptFamilyOtherBlues[i], ufoMasters[1].info.postscriptFamilyOtherBlues[i], interpolationFactor))))\n\t\tufoInstance.info.postscriptFamilyOtherBlues = tempArray\n\t# postscriptStemSnapH\n\tif len(ufoMasters[0].info.postscriptStemSnapH) == len(ufoMasters[1].info.postscriptStemSnapH):\n\t\tufoMasters[0].info.postscriptStemSnapH.sort()\n\t\tufoMasters[1].info.postscriptStemSnapH.sort()\n\t\ttempArray = []\n\t\tfor i in range(len(ufoMasters[0].info.postscriptStemSnapH)):\n\t\t\ttempArray.append(int(round(objectsBase._interpolate(ufoMasters[0].info.postscriptStemSnapH[i], ufoMasters[1].info.postscriptStemSnapH[i], interpolationFactor))))\n\t\tufoInstance.info.postscriptStemSnapH = tempArray\n\t# postscriptStemSnapV\n\tif len(ufoMasters[0].info.postscriptStemSnapV) == len(ufoMasters[1].info.postscriptStemSnapV):\n\t\tufoMasters[0].info.postscriptStemSnapV.sort()\n\t\tufoMasters[1].info.postscriptStemSnapV.sort()\n\t\ttempArray = []\n\t\tfor i in range(len(ufoMasters[0].info.postscriptStemSnapV)):\n\t\t\ttempArray.append(int(round(objectsBase._interpolate(ufoMasters[0].info.postscriptStemSnapV[i], ufoMasters[1].info.postscriptStemSnapV[i], interpolationFactor))))\n\t\tufoInstance.info.postscriptStemSnapV = tempArray\n\t\n\n\tfaceFolder = makeFaceFolder(outputDirPath, faceName)\n\tufoPath = os.path.join(faceFolder, kFontInstanceFileName)\n\n\t# Save UFO instance\n\tif not options.noUFOs:\n\t\tprint '\\tSaving %s file...' % kFontInstanceFileName\n\t\n\t\t# Delete the old UFO file, if it exists\n\t\twhile os.path.exists(ufoPath):\n\t\t\tshutil.rmtree(ufoPath)\n\t\n\t\tufoInstance.save(ufoPath)\n\t\n\t# Generate 'kern' feature\n\tif options.genKernFeature:\n\t\tprint \"\\tGenerating 'kern' feature...\"\n\t\tWriteFeaturesKernFDK.KernDataClass(ufoInstance, faceFolder, options.minKern, options.writeTrimmed, options.writeSubtables)\n\n\t# Generate 'mark/mkmk' features\n\tif options.genMarkFeature:\n\t\tif options.genMkmkFeature:\n\t\t\tprint \"\\tGenerating 'mark' and 'mkmk' features...\"\n\t\telse:\n\t\t\tprint \"\\tGenerating 'mark' feature...\"\n\t\tWriteFeaturesMarkFDK.MarkDataClass(ufoInstance, faceFolder, options.trimCasingTags, options.genMkmkFeature, options.writeClassesFile, options.indianScriptsFormat)\n\t\n\t\n\t# Decompose and remove overlaps (using checkoutlines)\n\tif options.flatten:\n\t\tprint '\\tFlattening the glyphs...'\n\t\tif os.name == \"nt\":\n\t\t\tcoTool = 'checkoutlines.cmd'\n\t\telse:\n\t\t\tcoTool = 'checkoutlines'\n\t\tcmd = '%s -e \"%s\"' % (coTool, ufoPath)\n\t\tpopen = Popen(cmd, shell=True, stdout=PIPE)\n\t\tpopenout, popenerr = popen.communicate()\n\t\tif options.verboseMode:\n\t\t\tif popenout:\n\t\t\t\tprint popenout\n\t\tif popenerr:\n\t\t\tprint popenerr\n\t\n\t# Autohint\n\tif options.autohint:\n\t\tprint '\\tHinting the font...'\n\t\tcmd = 'autohint -q \"%s\"' % ufoPath\n\t\tpopen = Popen(cmd, shell=True, stdout=PIPE)\n\t\tpopenout, popenerr = popen.communicate()\n\t\tif options.verboseMode:\n\t\t\tif popenout:\n\t\t\t\tprint popenout\n\t\tif popenerr:\n\t\t\tprint popenerr", "label": "IndexError", "info": "dataset/ETHPy150Open adobe-type-tools/python-scripts/UFOInstanceGenerator.py/makeInstance"}
{"function": "def getOptions(baseFolderPath):\n\toptions = Options()\n\toptions.inputPath = baseFolderPath\n\ti = 1\n\tnumOptions = len(sys.argv)\n\n\twhile i < numOptions:\n\t\targ = sys.argv[i]\n\n\t\tif arg == \"-h\":\n\t\t\tprint __help__\n\t\t\traise\n\t\telif arg == \"-u\":\n\t\t\tprint __usage__\n\t\t\traise\n\t\telif arg == \"-d\":\n\t\t\tprint __doc__\n\t\t\traise\n\t\telif arg == \"-v\":\n\t\t\toptions.verboseMode = True\n\t\telif arg == \"-o\":\n\t\t\ti += 1\n\t\t\ttry:\n\t\t\t\toutputPath = sys.argv[i]\n\t\t\texcept __HOLE__:\n\t\t\t\tprint >> sys.stderr, \"OPTION ERROR: It looks like the output folder was not specified.\"\n\t\t\t\traise\n\t\t\tif outputPath[0] == \"-\":\n\t\t\t\tprint >> sys.stderr, \"OPTION ERROR: It looks like the output folder was not specified.\"\n\t\t\t\traise\n\t\t\toptions.outputPath = outputPath\n\t\telif arg == \"-min\":\n\t\t\ti += 1\n\t\t\ttry:\n\t\t\t\tminimum = sys.argv[i]\n\t\t\texcept IndexError:\n\t\t\t\tprint >> sys.stderr, \"OPTION ERROR: It looks like the minimum value was not specified.\"\n\t\t\t\traise\n\t\t\tif minimum[0] == \"-\":\n\t\t\t\tprint >> sys.stderr, \"OPTION ERROR: It looks like the minimum value was not specified.\"\n\t\t\t\traise\n\t\t\ttry:\n\t\t\t\toptions.minKern = int(minimum)\n\t\t\texcept:\n\t\t\t\tprint >> sys.stderr, \"OPTION ERROR: It looks like the minimum value is not an integer.\"\n\t\t\t\traise\n\t\telif arg == \"-kern\":\n\t\t\toptions.genKernFeature = True\n\t\telif arg == \"-mark\":\n\t\t\toptions.genMarkFeature = True\n\t\telif arg == \"-nufo\":\n\t\t\toptions.noUFOs = True\n\t\telif arg == \"-wtr\":\n\t\t\toptions.writeTrimmed = True\n\t\telif arg == \"-wsb\":\n\t\t\toptions.writeSubtables = True\n\t\telif arg == \"-mkmk\":\n\t\t\toptions.genMkmkFeature = True\n\t\telif arg == \"-clas\":\n\t\t\toptions.writeClassesFile = True\n\t\telif arg == \"-indi\":\n\t\t\toptions.indianScriptsFormat = True\n\t\telif arg == \"-trtg\":\n\t\t\toptions.trimCasingTags = True\n\t\telif arg == \"-hint\":\n\t\t\toptions.autohint = True\n\t\telif arg == \"-flat\":\n\t\t\toptions.flatten = True\n\t\telif arg[0] == \"-\":\n\t\t\tprint >> sys.stderr, \"OPTION ERROR: Unknown option <%s>.\" %  arg\n\t\t\traise\n\t\ti  += 1\n\n\t# To do the 'mkmk' feature, the 'mark' feature must be done as well, therefore enable it\n\tif options.genMkmkFeature and not options.genMarkFeature:\n\t\toptions.genMarkFeature = True\n\n\treturn options", "label": "IndexError", "info": "dataset/ETHPy150Open adobe-type-tools/python-scripts/UFOInstanceGenerator.py/getOptions"}
{"function": "def run():\n\t# if an input path is provided\n\tif len(sys.argv[1:]) and sys.argv[1][0] != '-': # skip if it looks like an option\n\t\tbaseFolderPath = sys.argv[1]\n\n\t\tif baseFolderPath[-1] == '/':  # remove last slash if present\n\t\t\tbaseFolderPath = baseFolderPath[:-1]\n\n\t\t# make sure the path is valid\n\t\tif not os.path.isdir(baseFolderPath):\n\t\t\tprint >> sys.stderr, \"ERROR: Invalid input folder\\n\\t%s\" % baseFolderPath\n\t\t\treturn\n\n\t# if an input path is not provided, use the current directory\n\telse:\n\t\tbaseFolderPath = os.getcwd()\n\n\t# Load the options\n\ttry:\n\t\toptions = getOptions(baseFolderPath)\n\texcept:\n\t\treturn\n\t\n\t# Get paths to fonts and instances file\n\tfontPathsList, instancesFilePath = getFontPaths(baseFolderPath)\n\t\n\tif not len(fontPathsList):\n\t\tprint >> sys.stderr, \"ERROR: No UFO fonts were found in the path below\\n\\t%s\" % baseFolderPath\n\t\treturn\n\n\t# Handle the instances file\n\tif not instancesFilePath:\n\t\tprint >> sys.stderr, \"ERROR: Could not find the file named '%s' in the path below\\n\\t%s\" % (kInstancesDataFileName, baseFolderPath)\n\t\treturn\n\tprint \"Parsing %s file...\" % kInstancesDataFileName\n\ttry:\n\t\tinstancesList = readInstanceFile(instancesFilePath)\n\texcept:\n\t\tprint >> sys.stderr, \"ERROR: Error parsing file or file is empty.\"\n\t\treturn\n\n\t# Check the UFO file names\n\tmasterIndexes = []\n\tfor ufoPath in fontPathsList:\n\t\tfileNameNoExtension, fileExtension = os.path.splitext(ufoPath)\n\t\tmasterNumber = fileNameNoExtension.split('_')[-1]\n\t\t\n\t\tif masterNumber.isdigit():\n\t\t\tmasterIndexes.append(int(masterNumber))\n\tif masterIndexes != range(len(fontPathsList)):\n\t\tprint >> sys.stderr, \"ERROR: The UFO master files are not named properly\"\n\t\treturn\n\t\n\t# Check the number of UFOs against the number of axes in the instances file\n\taxisNum = int(math.log(len(masterIndexes), 2))\n\tfor i in range(len(instancesList)):\n\t\tinstanceDict = instancesList[i]\n\t\taxisVal = instanceDict[kCoordsKey] # Get AxisValues strings\n\t\tif axisNum != len(axisVal):\n\t\t\tprint 'ERROR:  The %s value for the instance named %s in the %s file is not compatible with the number of axis in the MM source font.' % (kCoordsKey, instanceDict[kFontName], kInstancesDataFileName)\n\t\t\treturn\n\n\t# Get the path to the output folder\n\ttry:\n\t\toutputDirPath = options.outputPath\n\t\tif not os.path.isdir(outputDirPath):\n\t\t\tprint >> sys.stderr, \"ERROR: Invalid output folder\\n\\t%s\" % outputDirPath\n\t\t\treturn\n\texcept __HOLE__: # use the current folder to output the instances\n\t\toutputDirPath = baseFolderPath\n\n\tt1 = time.time()\n\n\tprint \"Reading %d UFO files...\" % len(fontPathsList)\n\tufoMasters = [OpenFont(ufoPath) for ufoPath in fontPathsList]\t\n\n\ttotalInstances = len(instancesList)\n\tprint \"Generating %d instances...\" % totalInstances\n\tfor i in range(totalInstances):\n\t\tmakeInstance((i+1, totalInstances), ufoMasters, instancesList[i], outputDirPath, options)\n\n\tt2 = time.time()\n\telapsedSeconds = t2-t1\n\t\n\tif (elapsedSeconds/60) < 1:\n\t\tprint 'Completed in %.1f seconds.' % elapsedSeconds\n\telse:\n\t\tprint 'Completed in %.1f minutes.' % (elapsedSeconds/60)", "label": "AttributeError", "info": "dataset/ETHPy150Open adobe-type-tools/python-scripts/UFOInstanceGenerator.py/run"}
{"function": "def getPModuleClass(name):\n    global _pmodule_lookup\n    \n    try:\n        return _pmodule_lookup[name]\n    except __HOLE__:\n        raise NameError(\"Module '%s' not found.\" % name)", "label": "KeyError", "info": "dataset/ETHPy150Open hoytak/lazyrunner/lazyrunner/pmodule/lookup.py/getPModuleClass"}
{"function": "@property\n    def master_database(self):\n        try:\n            return self.config['master_database']\n        except __HOLE__:\n            return self.app.repository.master_branch.replace('-', '_')", "label": "KeyError", "info": "dataset/ETHPy150Open crosspop/asuka/asuka/services/pgsql.py/PostgreSQLService.master_database"}
{"function": "@property\n    def database(self):\n        try:\n            fmt = self.config['database_format']\n        except __HOLE__:\n            return self.branch.label_\n        return fmt.format(branch=self.branch)", "label": "KeyError", "info": "dataset/ETHPy150Open crosspop/asuka/asuka/services/pgsql.py/PostgreSQLService.database"}
{"function": "def was_modified_since(self, file_instance, since):\n        \"\"\"Delegate to file wrapper's was_modified_since, or return True.\n\n        This is the implementation of an edge case: when files are generated\n        on the fly, we cannot guess whether they have been modified or not.\n        If the file wrapper implements ``was_modified_since()`` method, then we\n        trust it. Otherwise it is safer to suppose that the file has been\n        modified.\n\n        This behaviour prevents file size to be computed on the Django side.\n        Because computing file size means iterating over all the file contents,\n        and we want to avoid that whenever possible. As an example, it could\n        reduce all the benefits of working with dynamic file generators...\n        which is a major feature of virtual files.\n\n        \"\"\"\n        try:\n            return file_instance.was_modified_since(since)\n        except (__HOLE__, NotImplementedError):\n            return True", "label": "AttributeError", "info": "dataset/ETHPy150Open benoitbryon/django-downloadview/django_downloadview/views/virtual.py/VirtualDownloadView.was_modified_since"}
{"function": "def find(self, path, all=False):\n        \"\"\"\n        Looks for files in the default file storage, if it's local.\n        \"\"\"\n        try:\n            self.storage.path('')\n        except __HOLE__:\n            pass\n        else:\n            if self.storage.exists(path):\n                match = self.storage.path(path)\n                if all:\n                    match = [match]\n                return match\n        return []", "label": "NotImplementedError", "info": "dataset/ETHPy150Open AppScale/appscale/AppServer/lib/django-1.4/django/contrib/staticfiles/finders.py/BaseStorageFinder.find"}
{"function": "def _get_finder(import_path):\n    \"\"\"\n    Imports the staticfiles finder class described by import_path, where\n    import_path is the full Python path to the class.\n    \"\"\"\n    module, attr = import_path.rsplit('.', 1)\n    try:\n        mod = import_module(module)\n    except __HOLE__, e:\n        raise ImproperlyConfigured('Error importing module %s: \"%s\"' %\n                                   (module, e))\n    try:\n        Finder = getattr(mod, attr)\n    except AttributeError:\n        raise ImproperlyConfigured('Module \"%s\" does not define a \"%s\" '\n                                   'class.' % (module, attr))\n    if not issubclass(Finder, BaseFinder):\n        raise ImproperlyConfigured('Finder \"%s\" is not a subclass of \"%s\"' %\n                                   (Finder, BaseFinder))\n    return Finder()", "label": "ImportError", "info": "dataset/ETHPy150Open AppScale/appscale/AppServer/lib/django-1.4/django/contrib/staticfiles/finders.py/_get_finder"}
{"function": "def __init__(self, Events=None, **Options):\n        \"\"\"Initializes the object.\n\n        :Parameters:\n          Events\n            An optional object with event handlers. See `Skype4Py.utils.EventHandlingBase`\n            for more information on events.\n          Options\n            Additional options for low-level API handler. See the `Skype4Py.api`\n            subpackage for supported options. Available options may depend on the\n            current platform. Note that the current platform can be queried using\n            `Skype4Py.platform` variable.\n        \"\"\"\n        self._Logger = logging.getLogger('Skype4Py.skype.Skype')\n        self._Logger.info('object created')\n\n        EventHandlingBase.__init__(self)\n        if Events:\n            self._SetEventHandlerObject(Events)\n\n        try:\n            self._Api = Options.pop('Api')\n            if Options:\n                raise TypeError('No options supported with custom API objects.')\n        except __HOLE__:\n            self._Api = SkypeAPI(Options)\n        self._Api.set_notifier(APINotifier(self))\n\n        Cached._CreateOwner(self)\n\n        self._Cache = True\n        self.ResetCache()\n\n        from api import DEFAULT_TIMEOUT\n        self._Timeout = DEFAULT_TIMEOUT\n\n        self._Convert = Conversion(self)\n        self._Client = Client(self)\n        self._Settings = Settings(self)\n        self._Profile = Profile(self)", "label": "KeyError", "info": "dataset/ETHPy150Open Skype4Py/Skype4Py/Skype4Py/skype.py/Skype.__init__"}
{"function": "def _Property(self, ObjectType, ObjectId, PropName, Set=None, Cache=True):\n        h = (str(ObjectType), str(ObjectId), str(PropName))\n        arg = ('%s %s %s' % h).split()\n        while '' in arg:\n            arg.remove('')\n        jarg = ' '.join(arg)\n        if Set is None: # Get\n            if Cache and self._Cache and h in self._CacheDict:\n                return self._CacheDict[h]\n            value = self._DoCommand('GET %s' % jarg, jarg)\n            while arg:\n                try:\n                    a, b = chop(value)\n                except __HOLE__:\n                    break\n                if a.lower() != arg[0].lower():\n                    break\n                del arg[0]\n                value = b\n            if Cache and self._Cache:\n                self._CacheDict[h] = value\n            return value\n        else: # Set\n            value = unicode(Set)\n            self._DoCommand('SET %s %s' % (jarg, value), jarg)\n            if Cache and self._Cache:\n                self._CacheDict[h] = value", "label": "ValueError", "info": "dataset/ETHPy150Open Skype4Py/Skype4Py/Skype4Py/skype.py/Skype._Property"}
{"function": "def _Alter(self, ObjectType, ObjectId, AlterName, Args=None, Reply=None):\n        cmd = 'ALTER %s %s %s' % (str(ObjectType), str(ObjectId), str(AlterName))\n        if Reply is None:\n            Reply = cmd\n        if Args is not None:\n            cmd = '%s %s' % (cmd, tounicode(Args))\n        reply = self._DoCommand(cmd, Reply)\n        arg = cmd.split()\n        while arg:\n            try:\n                a, b = chop(reply)\n            except __HOLE__:\n                break\n            if a.lower() != arg[0].lower():\n                break\n            del arg[0]\n            reply = b\n        return reply", "label": "ValueError", "info": "dataset/ETHPy150Open Skype4Py/Skype4Py/Skype4Py/skype.py/Skype._Alter"}
{"function": "def get_state(self):\n        \"\"\"\n        Get the state of the snapshot\n        \"\"\"\n        try:\n            self.state = self.client.snapshot.get(\n                repository=self.repository,\n                snapshot=self.name)['snapshots'][0]['state']\n            return self.state\n        except __HOLE__:\n            raise CuratorException(\n                'Snapshot \"{0}\" not found in repository '\n                '\"{1}\"'.format(self.name, self.repository)\n            )", "label": "IndexError", "info": "dataset/ETHPy150Open elastic/curator/curator/actions.py/Snapshot.get_state"}
{"function": "def _walk(root, top, ignore_files=None, ignore_dirs=None):\n    dirs_to_visit = []\n\n    try:\n        dir_list = sorted(listdir(join(root, top)), key=lambda r: (len(r), r))\n    except __HOLE__:\n        pass\n    else:\n        for name in dir_list:\n            path = join(top, name)\n            fullpath = join(root, top, name)\n            if isdir(fullpath):\n                if not ignore_dirs or not ignore_dirs.match(path):\n                    dirs_to_visit.append(path)\n            else:\n                if not ignore_files or not ignore_files.match(path):\n                    yield name, path, root, top, fullpath\n\n    for path in dirs_to_visit:\n        for p in _walk(root, path, ignore_files, ignore_dirs):\n            yield p", "label": "OSError", "info": "dataset/ETHPy150Open baverman/vial/vial/fsearch.py/_walk"}
{"function": "def get_files(root, cache=None, keep_top=False):\n    if cache is not None:\n        try:\n            return cache[root]\n        except __HOLE__:\n            pass\n\n    ignore_files = re.compile('.*({})$'.format(\n        '|'.join(r'\\.{}'.format(r)\n                 for r in get_dvar('vial_ignore_extensions'))))\n\n    ignore_dirs = re.compile('({})'.format('|'.join(\n        get_dvar('vial_ignore_dirs'))))\n\n    fcache = []\n    if keep_top:\n        root, top = split(root)\n    else:\n        root, top = root, ''\n\n    def filler():\n        for r in _walk(root, top, ignore_files, ignore_dirs):\n            fcache.append(r)\n            yield r\n\n        if cache is not None:\n            cache[root] = fcache\n\n    return filler()", "label": "KeyError", "info": "dataset/ETHPy150Open baverman/vial/vial/fsearch.py/get_files"}
